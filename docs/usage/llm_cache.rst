

=========
LLM Cache
=========

LLM Caching is the process by which interactions with the LLM are cached in
a database capable of performing vector search. This allows for the LLM to
find similar interactions to those that have been previously seen.

