{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and Answer with OpenAI and RedisVL\n",
    "\n",
    "This example shows how to use RedisVL to create a question and answer system using OpenAI's API.\n",
    "\n",
    "In this notebook we will\n",
    "1. Download a dataset of wikipedia articles (thanks to OpenAI's CDN)\n",
    "2. Create embeddings for each article\n",
    "3. Create a RedisVL index and store the embeddings with metadata\n",
    "4. Construct a simple QnA system using the index and GPT-3\n",
    "5. Improve the QnA system with LLMCache\n",
    "\n",
    "\n",
    "The image below shows the architecture of the system we will create in this notebook.\n",
    "\n",
    "![Diagram](https://github.com/RedisVentures/redis-openai-qna/raw/main/app/assets/RedisOpenAI-QnA-Architecture.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In order to run this example, you will need to have a Redis instance with RediSearch running locally. You can do this by running the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "docker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```\n",
    "\n",
    "This will also provide the RedisInsight GUI at http://localhost:8001\n",
    "\n",
    "Next, we will install the dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to install a few things\n",
    "\n",
    "%pip install pandas wget tenacity tiktoken openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'redisvl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/tyler.hutcherson/RedisVentures/redisvl/docs/examples/openai_qna.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tyler.hutcherson/RedisVentures/redisvl/docs/examples/openai_qna.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mredisvl\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'redisvl'"
     ]
    }
   ],
   "source": [
    "import redisvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "embeddings_url = 'https://cdn.openai.com/API/examples/data/wikipedia_articles_2000.csv'\n",
    "\n",
    "wget.download(embeddings_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wikipedia_articles_2000.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Chunking\n",
    "\n",
    "In order to create embeddings for the articles, we will need to chunk the text into smaller pieces. This is because there is a maximum length of text that can be sent to the OpenAI API. The code that follows pulls heavily from this [notebook](https://github.com/openai/openai-cookbook/blob/main/apps/enterprise-knowledge-retrieval/enterprise_knowledge_retrieval.ipynb) by OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_CHUNK_SIZE = 1000\n",
    "EMBEDDINGS_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "def chunks(text, n, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \"\"\"Yield successive n-sized chunks from text.\n",
    "\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "def get_unique_id_for_file_chunk(title, chunk_index):\n",
    "    return str(title+\"-!\"+str(chunk_index))\n",
    "\n",
    "def chunk_text(record, tokenizer):\n",
    "    chunked_records = []\n",
    "\n",
    "    url = record['url']\n",
    "    title = record['title']\n",
    "    file_body_string = record['text']\n",
    "\n",
    "    \"\"\"Return a list of tuples (text_chunk, embedding) for a text.\"\"\"\n",
    "    token_chunks = list(chunks(file_body_string, TEXT_EMBEDDING_CHUNK_SIZE, tokenizer))\n",
    "    text_chunks = [f'Title: {title};\\n'+ tokenizer.decode(chunk) for chunk in token_chunks]\n",
    "\n",
    "    for i, text_chunk in enumerate(text_chunks):\n",
    "        doc_id = get_unique_id_for_file_chunk(title, i)\n",
    "        chunked_records.append(({\"id\": doc_id,\n",
    "                                \"url\": url,\n",
    "                                \"title\": title,\n",
    "                                \"content\": text_chunk,\n",
    "                                \"file_chunk_index\": i}))\n",
    "    return chunked_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tokenizer\n",
    "import tiktoken\n",
    "oai_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "records = []\n",
    "for _, record in df.iterrows():\n",
    "    records.extend(chunk_text(record, oai_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = pd.DataFrame(records)\n",
    "chunked_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Creation\n",
    "\n",
    "With the text broken up into chunks, we can create embeddings with the RedisVL `OpenAITextVectorizer`. This provider uses the OpenAI API to create embeddings for the text. The code below shows how to create embeddings for the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from redisvl.vectorize.text import OpenAITextVectorizer\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "oaip = OpenAITextVectorizer(EMBEDDINGS_MODEL, api_config={\"api_key\": api_key})\n",
    "\n",
    "chunked_data[\"embedding\"] = oaip.embed_many(chunked_data[\"content\"].tolist(), as_buffer=True)\n",
    "chunked_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the ``SearchIndex``\n",
    "\n",
    "Now that we have the embeddings, we can create a ``SearchIndex`` to store them in Redis. We will use the ``SearchIndex`` to store the embeddings and metadata for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wiki_schema.yaml\n",
    "\n",
    "index:\n",
    "    name: wiki\n",
    "    prefix: oaiWiki\n",
    "\n",
    "fields:\n",
    "    text:\n",
    "        - name: content\n",
    "        - name: title\n",
    "    tag:\n",
    "        - name: id\n",
    "    vector:\n",
    "        - name: embedding\n",
    "          dims: 1536\n",
    "          distance_metric: cosine\n",
    "          algorithm: flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.index import AsyncSearchIndex\n",
    "\n",
    "index = AsyncSearchIndex.from_yaml(\"wiki_schema.yaml\")\n",
    "index.connect(\"redis://localhost:6379\")\n",
    "\n",
    "await index.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rvl index listall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await index.load(chunked_data.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the QnA System\n",
    "\n",
    "Now that we have the data and the embeddings, we can build the QnA system. The system will perform three actions\n",
    "\n",
    "1. Embed the user question and search for the most similar content\n",
    "2. Make a prompt with the query and retrieved content\n",
    "3. Send the prompt to the OpenAI API and return the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from redisvl.query import VectorQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "def make_prompt(query, content):\n",
    "    retrieval_prompt = f'''Use the content to answer the search query the customer has sent.\n",
    "    If you can't answer the user's question, do not guess. If there is no content, respond with \"I don't know\".\n",
    "\n",
    "    Search query:\n",
    "\n",
    "    {query}\n",
    "\n",
    "    Content:\n",
    "\n",
    "    {content}\n",
    "\n",
    "    Answer:\n",
    "    '''\n",
    "    return retrieval_prompt\n",
    "\n",
    "async def retrieve_context(query):\n",
    "    # Embed the query\n",
    "    query_embedding = oaip.embed(query)\n",
    "\n",
    "    # Get the top result from the index\n",
    "    vector_query = VectorQuery(\n",
    "        vector=query_embedding,\n",
    "        vector_field_name=\"embedding\",\n",
    "        return_fields=[\"content\"],\n",
    "        num_results=1\n",
    "    )\n",
    "\n",
    "    results = await index.query(vector_query)\n",
    "    content = \"\"\n",
    "    if len(results) > 1:\n",
    "        content = results[0][\"content\"]\n",
    "    return content\n",
    "\n",
    "async def answer_question(query):\n",
    "    # Retrieve the context\n",
    "    content = await retrieve_context(query)\n",
    "\n",
    "    prompt = make_prompt(query, content)\n",
    "    retrieval = await openai.ChatCompletion.acreate(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{'role':\"user\",\n",
    "                   'content': prompt}],\n",
    "        max_tokens=500)\n",
    "\n",
    "    # Response provided by GPT-3.5\n",
    "    return retrieval['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "question = \"What is a Brontosaurus?\"\n",
    "textwrap.wrap(await answer_question(question), width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question that makes no sense\n",
    "question = \"What is a trackiosamidon?\"\n",
    "await answer_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about the life of Alanis Morissette\"\n",
    "textwrap.wrap(await answer_question(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the QnA System with LLMCache\n",
    "\n",
    "The QnA system we built above is pretty good, but it can be improved. We can use the ``LLMCache`` to improve the system. The ``LLMCache`` will store the results of previous queries and return them if the query is similar enough to a previous query. This will reduce the number of queries we need to send to the OpenAI API and increase the overall QPS of the system assuming we expect similar queries to be asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.llmcache.semantic import SemanticCache\n",
    "\n",
    "cache = SemanticCache(redis_url=\"redis://localhost:6379\", threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_question(query):\n",
    "\n",
    "    # check the cache\n",
    "    result = cache.check(prompt=query)\n",
    "    if result:\n",
    "        return result[0]\n",
    "\n",
    "    # Retrieve the context\n",
    "    content = await retrieve_context(query)\n",
    "\n",
    "    prompt = make_prompt(query, content)\n",
    "    retrieval = await openai.ChatCompletion.acreate(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{'role':\"user\",\n",
    "                   'content': prompt}],\n",
    "        max_tokens=500)\n",
    "\n",
    "    # Response provided by GPT-3.5\n",
    "    answer = retrieval['choices'][0]['message']['content']\n",
    "\n",
    "    # cache the query_embedding and answer\n",
    "    cache.store(query, answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a question to cache an answer\n",
    "import time\n",
    "start = time.time()\n",
    "question = \"Tell me about the life of Alanis Morissette\"\n",
    "answer = await answer_question(question)\n",
    "print(f\"Time taken: {time.time() - start}\\n\")\n",
    "textwrap.wrap(answer, width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same question, return cached answer, save time, save money :)\n",
    "start = time.time()\n",
    "answer = await answer_question(question)\n",
    "print(f\"Time taken with cache: {time.time() - start}\\n\")\n",
    "textwrap.wrap(answer, width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask a semantically similar question returns the same answer from the cache\n",
    "# but isn't exactly the same question. In this case, the semantic similarity between\n",
    "# the questions is greater than the threshold of 0.8 the cache is set to.\n",
    "start = time.time()\n",
    "question = \"Who is Alanis Morissette?\"\n",
    "answer = await answer_question(question)\n",
    "print(f\"Time taken with the cache: {time.time() - start}\\n\")\n",
    "textwrap.wrap(answer, width=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
